import gc
import os
import time
import lightgbm as lgb
from lightgbm.callback import early_stopping
from catboost import CatBoostClassifier
import xgboost as xgb
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
import glob
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
import pickle

def load_data(data_type):
    """ë°ì´í„° íŒŒì¼ ë¡œë“œ í•¨ìˆ˜"""
    base_path = f"open/{data_type}"
    categories = {
        "customer": "1",
        "credit": "2",
        "sales": "3",
        "billing": "4",
        "balance": "5",
        "channel": "6",
        "marketing": "7",
        "performance": "8"
    }
    dfs = {}
    for name, prefix in categories.items():
        key = f"{name}_{data_type}_df"
        path = f"{base_path}/{prefix}.*/*.parquet"
        files = sorted(glob.glob(path))
        if not files:
            print(f"{key}: {path} ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue
        dfs[key] = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)
        print(f"{key} ë¡œë“œ ì™„ë£Œ: {dfs[key].shape}")
    return dfs


def merge_dataframes(base_df, merge_keys, merge_items, prefix="train", dfs=None):
    """ë°ì´í„°í”„ë ˆì„ ë³‘í•© í•¨ìˆ˜"""
    for df_name, step in merge_items:
        full_name = f"{df_name}_{prefix}_df"
        if full_name in dfs:
            print(f"{step} ë³‘í•© ì¤‘: {full_name}")
            base_df = base_df.merge(dfs[full_name], on=merge_keys, how='left')
            print(f"{step} ë³‘í•© ì™„ë£Œ: shape â†’ {base_df.shape}")
            del dfs[full_name]
            gc.collect()
        else:
            print(f"{full_name} ì—†ìŒ - ë³‘í•© ìƒëµ")
    return base_df


def convert_text_to_int(df, exclude_cols=['ID', 'ê¸°ì¤€ë…„ì›”']):
    """í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    object_cols = df.select_dtypes(include='object').columns
    target_cols = [col for col in object_cols if col not in exclude_cols and col != 'Segment']

    if not target_cols:
        return df

    for col in target_cols:
        try:
            # ìˆ«ì ì¶”ì¶œ íŒ¨í„´ ê°œì„ : ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°ì™€ ìˆ«ì+ë‹¨ìœ„ê°€ ìˆëŠ” ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            num_series = df[col].astype(str).str.extract(r'(\d+)(?:ê°œ|ëŒ€|íšŒ|ì´ìƒ|íšŒì´ìƒ|ê°œì´ìƒ|ëŒ€ì´ìƒ)?')[0]
            num_series = pd.to_numeric(num_series, errors='coerce')

            # ìˆ«ì ë³€í™˜ì— ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì ìš©
            if num_series.notna().sum() > 0:
                # ë³€í™˜ ì‹¤íŒ¨í•œ ê²½ìš°ëŠ” ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
                median_val = num_series[num_series.notna()].median()
                num_series = num_series.fillna(median_val)
                df[col] = num_series

            # ë³€í™˜ ì‹¤íŒ¨í•œ ì»¬ëŸ¼ì€ ë²”ì£¼í˜•ìœ¼ë¡œ ì²˜ë¦¬
            else:
                print(f"ê²½ê³ : {col} ì»¬ëŸ¼ì€ ìˆ«ì ë³€í™˜ì— ì‹¤íŒ¨í•˜ì—¬ ë²”ì£¼í˜•ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        except:
            print(f"ê²½ê³ : {col} ì»¬ëŸ¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

    return df


def reduce_mem_usage(df, verbose=True):
    """ë°ì´í„°í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” í•¨ìˆ˜"""
    start_mem = df.memory_usage().sum() / 1024 ** 2

    for col in df.columns:
        if col in ['ID', 'Segment']:  # IDì™€ íƒ€ê²Ÿ ì»¬ëŸ¼ì€ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
            continue

        col_type = df[col].dtype

        if col_type != object:  # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼
            c_min = df[col].min()
            c_max = df[col].max()

            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                else:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)

    end_mem = df.memory_usage().sum() / 1024 ** 2
    reduction = 100 * (start_mem - end_mem) / start_mem

    if verbose:
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {start_mem:.2f} MB â†’ {end_mem:.2f} MB ({reduction:.1f}% ê°ì†Œ)")

    return df


def merge_dataframes(base_df, merge_keys, merge_items, prefix="train", dfs=None):
    """ë°ì´í„°í”„ë ˆì„ ë³‘í•© í•¨ìˆ˜"""
    for df_name, step in merge_items:
        full_name = f"{df_name}_{prefix}_df"
        if full_name in dfs:
            print(f"{step} ë³‘í•© ì¤‘: {full_name}")
            base_df = base_df.merge(dfs[full_name], on=merge_keys, how='left')
            print(f"{step} ë³‘í•© ì™„ë£Œ: shape â†’ {base_df.shape}")
            del dfs[full_name]
            gc.collect()
        else:
            print(f"{full_name} ì—†ìŒ - ë³‘í•© ìƒëµ")
    return base_df


def convert_text_to_int(df, exclude_cols=['ID', 'ê¸°ì¤€ë…„ì›”']):
    """í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    object_cols = df.select_dtypes(include='object').columns
    target_cols = [col for col in object_cols if col not in exclude_cols and col != 'Segment']

    if not target_cols:
        return df

    for col in target_cols:
        try:
            # ìˆ«ì ì¶”ì¶œ íŒ¨í„´ ê°œì„ : ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°ì™€ ìˆ«ì+ë‹¨ìœ„ê°€ ìˆëŠ” ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            num_series = df[col].astype(str).str.extract(r'(\d+)(?:ê°œ|ëŒ€|íšŒ|ì´ìƒ|íšŒì´ìƒ|ê°œì´ìƒ|ëŒ€ì´ìƒ)?')[0]
            num_series = pd.to_numeric(num_series, errors='coerce')

            # ìˆ«ì ë³€í™˜ì— ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì ìš©
            if num_series.notna().sum() > 0:
                # ë³€í™˜ ì‹¤íŒ¨í•œ ê²½ìš°ëŠ” ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
                median_val = num_series[num_series.notna()].median()
                num_series = num_series.fillna(median_val)
                df[col] = num_series

            # ë³€í™˜ ì‹¤íŒ¨í•œ ì»¬ëŸ¼ì€ ë²”ì£¼í˜•ìœ¼ë¡œ ì²˜ë¦¬
            else:
                print(f"ê²½ê³ : {col} ì»¬ëŸ¼ì€ ìˆ«ì ë³€í™˜ì— ì‹¤íŒ¨í•˜ì—¬ ë²”ì£¼í˜•ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        except:
            print(f"ê²½ê³ : {col} ì»¬ëŸ¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

    return df


def reduce_mem_usage(df, verbose=True):
    """ë°ì´í„°í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” í•¨ìˆ˜"""
    start_mem = df.memory_usage().sum() / 1024 ** 2

    for col in df.columns:
        if col in ['ID', 'Segment']:  # IDì™€ íƒ€ê²Ÿ ì»¬ëŸ¼ì€ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
            continue

        col_type = df[col].dtype

        if col_type != object:  # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼
            c_min = df[col].min()
            c_max = df[col].max()

            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                else:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)

    end_mem = df.memory_usage().sum() / 1024 ** 2
    reduction = 100 * (start_mem - end_mem) / start_mem

    if verbose:
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {start_mem:.2f} MB â†’ {end_mem:.2f} MB ({reduction:.1f}% ê°ì†Œ)")

    return df


def handle_numeric_features(train_df, test_df):
    """ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì²˜ë¦¬ í•¨ìˆ˜"""
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì‹ë³„
    numeric_cols = train_df.select_dtypes(include=['int', 'float']).columns.tolist()

    # ê²°ì¸¡ì¹˜ ë° ì´ìƒê°’ ì²˜ë¦¬
    for col in numeric_cols:
        # íŠ¹ìˆ˜ ê°’ ì²˜ë¦¬ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        replace_values = [10101, -999999, -99, 999, 99999999]
        train_df[col] = train_df[col].replace(replace_values, np.nan)
        test_df[col] = test_df[col].replace(replace_values, np.nan)

        # í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ ì„ê³„ê°’ ê³„ì‚° (25%, 75% ë¶„ìœ„ìˆ˜ë¡œ ë³€ê²½)
        q1 = train_df[col].quantile(0.25)
        q3 = train_df[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr

        # ì´ìƒì¹˜ í´ë¦¬í•‘
        orig_dtype = train_df[col].dtype
        train_df[col] = train_df[col].clip(lower=lower_bound, upper=upper_bound).astype(orig_dtype)
        test_df[col] = test_df[col].clip(lower=lower_bound, upper=upper_bound).astype(orig_dtype)

        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
        median_value = train_df[col].median()
        train_df[col] = train_df[col].fillna(median_value)
        test_df[col] = test_df[col].fillna(median_value)

    return train_df, test_df


def drop_useless_features(train_df, test_df):
    """ë¶ˆí•„ìš”í•œ í”¼ì²˜ ì œê±° í•¨ìˆ˜"""
    # ìƒìˆ˜ ì»¬ëŸ¼ ì‹ë³„ (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€)
    constant_cols = [col for col in train_df.columns
                     if train_df[col].nunique(dropna=False) <= 1 or
                     (col != 'Segment' and
                      train_df[col].dtype in [np.int64, np.float64] and
                      train_df[col].std() < 1e-6)]

    domain_knowledge_drop = [
        # ì—¬ê¸°ì— ë„ë©”ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì œê±°í•˜ë ¤ëŠ” ì»¬ëŸ¼ ì´ë¦„ì„ ë¬¸ìì—´ë¡œ ì¶”ê°€
        'ë‚¨ë…€êµ¬ë¶„ì½”ë“œ', 'ê°€ì…í†µì‹ íšŒì‚¬ì½”ë“œ',
        'ì´ìš©ê°€ëŠ¥ì¹´ë“œìˆ˜_ì²´í¬', 'ì´ìš©ê°€ëŠ¥ì¹´ë“œìˆ˜_ì²´í¬_ê°€ì¡±',
        'ìˆ˜ì‹ ê±°ë¶€ì—¬ë¶€_TM', 'ìˆ˜ì‹ ê±°ë¶€ì—¬ë¶€_DM', 'ìˆ˜ì‹ ê±°ë¶€ì—¬ë¶€_ë©”ì¼', 'ìˆ˜ì‹ ê±°ë¶€ì—¬ë¶€_SMS',
        'ë§ˆì¼€íŒ…ë™ì˜ì—¬ë¶€', 'ìµœì¢…íƒˆíšŒê²½ê³¼ì¼',
        'í• ì¸ê¸ˆ_ê¸°ë³¸ì—°íšŒ_BOM', 'Life_Stage', 'ìµœì¢…ì¹´ë“œë°œê²½ê³¼ì›”',
        '_1ìˆœìœ„ì‡¼í•‘ì—…ì¢…', '_1ìˆœìœ„ì‡¼í•‘ì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_2ìˆœìœ„ì‡¼í•‘ì—…ì¢…', '_2ìˆœìœ„ì‡¼í•‘ì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_3ìˆœìœ„ì‡¼í•‘ì—…ì¢…', '_3ìˆœìœ„ì‡¼í•‘ì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_1ìˆœìœ„êµí†µì—…ì¢…', '_1ìˆœìœ„êµí†µì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_2ìˆœìœ„êµí†µì—…ì¢…', '_2ìˆœìœ„êµí†µì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_3ìˆœìœ„êµí†µì—…ì¢…', '_3ìˆœìœ„êµí†µì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_1ìˆœìœ„ì—¬ìœ ì—…ì¢…', '_1ìˆœìœ„ì—¬ìœ ì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_2ìˆœìœ„ì—¬ìœ ì—…ì¢…', '_2ìˆœìœ„ì—¬ìœ ì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_3ìˆœìœ„ì—¬ìœ ì—…ì¢…', '_3ìˆœìœ„ì—¬ìœ ì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_1ìˆœìœ„ë‚©ë¶€ì—…ì¢…', '_1ìˆœìœ„ë‚©ë¶€ì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_2ìˆœìœ„ë‚©ë¶€ì—…ì¢…', '_2ìˆœìœ„ë‚©ë¶€ì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        '_3ìˆœìœ„ë‚©ë¶€ì—…ì¢…', '_3ìˆœìœ„ë‚©ë¶€ì—…ì¢…_ì´ìš©ê¸ˆì•¡',
        'ìë°œí•œë„ê°ì•¡íšŸìˆ˜_R12M', 'ìë°œí•œë„ê°ì•¡ê¸ˆì•¡_R12M', 'ìë°œí•œê°ì•¡í›„ê²½ê³¼ì›”',
        'ìµœì¢…ì¹´ë“œë¡ _ê¸ˆìœµìƒí™˜ë°©ì‹ì½”ë“œ',
        'ìµœì¢…ì¹´ë“œë¡ _ì‹ ì²­ê²½ë¡œì½”ë“œ',
        'ìµœì¢…ì¹´ë“œë¡ _ëŒ€ì¶œì¼ì'
        'ëŒ€í‘œê²°ì œì¼',
        'í¬ì¸íŠ¸_ë§ˆì¼ë¦¬_í™˜ì‚°_BOM',
        'ì¸ì…íšŸìˆ˜_ARS_R6M',
        'ì´ìš©ë©”ê±´ìˆ˜_ARS_R6M',
        'ì»¨íƒê±´ìˆ˜_CA_ë‹¹ì‚¬ì•±_R6M',
        'ìº í˜ì¸ì ‘ì´‰ê±´ìˆ˜_R12M', 'ìº í˜ì¸ì ‘ì´‰ì¼ìˆ˜_R12M',
        'ëŒ€í‘œê²°ì œë°©ë²•ì½”ë“œ', 'ëŒ€í‘œì²­êµ¬ì§€ê³ ê°ì£¼ì†Œêµ¬ë¶„ì½”ë“œ',
        'ì…íšŒê²½ê³¼ê°œì›”ìˆ˜_ì‹ ìš©', 'ìˆ˜ì‹ ê±°ë¶€ì—¬ë¶€_ë©”ì¼',
        'ì´ìš©ê°€ëŠ¥ì¹´ë“œìˆ˜_ì²´í¬', 'ì—°ì²´ì¼ì_B0M', 'OSêµ¬ë¶„ì½”ë“œ',
    ]

    # ì‹¤ì œë¡œ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
    domain_knowledge_drop = [col for col in domain_knowledge_drop if col in train_df.columns]

    # ëª¨ë“  ì œê±° ëŒ€ìƒ ì»¬ëŸ¼ í†µí•©
    all_cols_to_drop = list(set(constant_cols + domain_knowledge_drop))

    if all_cols_to_drop:
        print(f"ì œê±°í•  í”¼ì²˜ ì´ {len(all_cols_to_drop)}ê°œ")
        print(f"- ìƒìˆ˜/ì €ë¶„ì‚° ì»¬ëŸ¼ ({len(constant_cols)}ê°œ): {constant_cols}")
        print(f"- ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì œê±° ({len(domain_knowledge_drop)}ê°œ): {domain_knowledge_drop}")

        train_df = train_df.drop(columns=all_cols_to_drop)
        test_df = test_df.drop(columns=[col for col in all_cols_to_drop if col in test_df.columns])

    return train_df, test_df


def drop_highly_correlated_features(train_df, test_df, threshold=0.95):
    """ìƒê´€ê´€ê³„ ë†’ì€ í”¼ì²˜ ì œê±° í•¨ìˆ˜"""
    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
    numeric_cols = train_df.select_dtypes(include=['int', 'float']).columns

    # ìƒê´€ê´€ê³„ ê³„ì‚°
    if len(numeric_cols) > 1:  # ì ì–´ë„ 2ê°œ ì´ìƒì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨
        corr_matrix = train_df[numeric_cols].corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]

        if to_drop:
            print(f"ğŸ’¡ ìƒê´€ê´€ê³„ {threshold} ì´ˆê³¼ í”¼ì²˜ ì œê±°: {to_drop}")
            train_df = train_df.drop(columns=to_drop)
            test_df = test_df.drop(columns=[col for col in to_drop if col in test_df.columns])

    return train_df, test_df


def handle_missing_values(X, X_test):
    """ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜"""
    for col in X.columns:
        if X[col].isna().sum() > 0:
            if X[col].dtype.kind in 'ifc':  # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë©´ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
                median_val = X[col].median()
                X[col] = X[col].fillna(median_val)
                X_test[col] = X_test[col].fillna(median_val)
            else:  # ë²”ì£¼í˜• ë³€ìˆ˜ë©´ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
                mode_val = X[col].mode()[0]
                X[col] = X[col].fillna(mode_val)
                X_test[col] = X_test[col].fillna(mode_val)

    return X, X_test


def encode_categorical_features(X, X_test, categorical_features):
    """ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© í•¨ìˆ˜"""
    encoders = {}

    for col in categorical_features:
        if col not in X.columns or col not in X_test.columns:
            print(f"ê²½ê³ : {col} ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.")
            continue

        le = LabelEncoder()
        # í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ í•©ì³ì„œ ì¸ì½”ë”©
        all_categories = pd.concat([X[col].astype(str), X_test[col].astype(str)]).unique()
        le.fit(all_categories)

        X[col] = le.transform(X[col].astype(str))
        X_test[col] = le.transform(X_test[col].astype(str))
        encoders[col] = le

    return X, X_test, encoders


def preprocess_data(reload_from_parquet=True):
    """ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜

    Args:
        reload_from_parquet (bool): Trueì¼ ê²½ìš° ì´ë¯¸ ì „ì²˜ë¦¬ëœ Parquet íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ì²˜ìŒë¶€í„° ì „ì²˜ë¦¬

    Returns:
        tuple: (train_df, test_df) ì „ì²˜ë¦¬ëœ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    """
    # ì´ë¯¸ ì „ì²˜ë¦¬ëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
    preprocessed_train_path = "preprocessed_train_data.parquet"
    preprocessed_test_path = "preprocessed_test_data.parquet"

    if reload_from_parquet and os.path.exists(preprocessed_train_path) and os.path.exists(preprocessed_test_path):
        print(f"ì´ë¯¸ ì „ì²˜ë¦¬ëœ íŒŒì¼ ë°œê²¬: {preprocessed_train_path}, {preprocessed_test_path}")
        print("ì „ì²˜ë¦¬ëœ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        train_df = pd.read_parquet(preprocessed_train_path)
        test_df = pd.read_parquet(preprocessed_test_path)
        print(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ! í›ˆë ¨ ë°ì´í„°: {train_df.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_df.shape}")
        return train_df, test_df

    print("ì „ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ê±°ë‚˜ ì¬ì „ì²˜ë¦¬ê°€ ìš”ì²­ë˜ì–´ ì²˜ìŒë¶€í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
    print("ë°ì´í„° ë¡œë”© ì‹œì‘...")
    train_dfs = load_data("train")
    test_dfs = load_data("test")
    print("ë°ì´í„° ë¡œë”© ì™„ë£Œ!")

    merge_list = [
        ("sales", "Step2"),
        ("billing", "Step3"),
        ("balance", "Step4"),
        ("channel", "Step5"),
        ("marketing", "Step6"),
        ("performance", "ìµœì¢…")
    ]

    print("ë°ì´í„° ë³‘í•© ì‹œì‘...")
    train_df = train_dfs["customer_train_df"].merge(train_dfs["credit_train_df"], on=["ê¸°ì¤€ë…„ì›”", "ID"], how="left")
    test_df = test_dfs["customer_test_df"].merge(test_dfs["credit_test_df"], on=["ê¸°ì¤€ë…„ì›”", "ID"], how="left")

    train_df = merge_dataframes(train_df, ["ê¸°ì¤€ë…„ì›”", "ID"], merge_list, prefix="train", dfs=train_dfs)
    test_df = merge_dataframes(test_df, ["ê¸°ì¤€ë…„ì›”", "ID"], merge_list, prefix="test", dfs=test_dfs)
    print("ë°ì´í„° ë³‘í•© ì™„ë£Œ!")

    print("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ì¤‘...")
    train_df = reduce_mem_usage(train_df)
    test_df = reduce_mem_usage(test_df)

    # ë³‘í•©ëœ ë°ì´í„° Parquetìœ¼ë¡œ ì €ì¥ (ì¤‘ê°„ ê²°ê³¼)
    print("ë³‘í•©ëœ ë°ì´í„° Parquet íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
    train_df.to_parquet("merged_train_data.parquet", index=False)
    test_df.to_parquet("merged_test_data.parquet", index=False)
    print("ë³‘í•©ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ! íŒŒì¼ëª…: merged_train_data.parquet, merged_test_data.parquet")

    print("í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜ ì¤‘...")
    train_df = convert_text_to_int(train_df)
    test_df = convert_text_to_int(test_df)

    print("ìˆ˜ì¹˜í˜• ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    train_df, test_df = handle_numeric_features(train_df, test_df)

    print("ë¶ˆí•„ìš”í•œ í”¼ì²˜ ì œê±° ì¤‘...")
    train_df, test_df = drop_useless_features(train_df, test_df)

    print("ìƒê´€ê´€ê³„ ë†’ì€ í”¼ì²˜ ì œê±° ì¤‘...")
    train_df, test_df = drop_highly_correlated_features(train_df, test_df)

    # ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„° ì €ì¥
    print("ì „ì²˜ë¦¬ëœ ë°ì´í„° Parquet íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
    train_df.to_parquet(preprocessed_train_path, index=False)
    test_df.to_parquet(preprocessed_test_path, index=False)
    print(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ! íŒŒì¼ëª…: {preprocessed_train_path}, {preprocessed_test_path}")

    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ! í›ˆë ¨ ë°ì´í„°: {train_df.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_df.shape}")
    return train_df, test_df


def scale_features(X_train, X_test, scaler=None):
    """ìˆ˜ì¹˜í˜• íŠ¹ì„± ìŠ¤ì¼€ì¼ë§"""
    numeric_cols = X_train.select_dtypes(include=['int', 'float']).columns

    if scaler is None:
        scaler = StandardScaler()
        fit_scaler = True
    else:
        fit_scaler = False

    X_train_scaled = X_train.copy()
    X_test_scaled = X_test.copy()

    if fit_scaler:
        X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
    else:
        X_train_scaled[numeric_cols] = scaler.transform(X_train[numeric_cols])

    X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])

    return X_train_scaled, X_test_scaled, scaler


def train_and_evaluate_models(X, y, X_test, test_ids, test_df, n_folds=5, random_state=42):
    """ëª¨ë¸ í•™ìŠµ ë° êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    # ì „ì²´ í•™ìŠµ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    total_start_time = time.time()

    # êµì°¨ ê²€ì¦ ì„¤ì •
    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)

    # ê° ëª¨ë¸ë³„ OOF ì˜ˆì¸¡ ê²°ê³¼
    oof_preds = {
        'lightgbm': np.zeros((X.shape[0], len(np.unique(y)))),
        'xgboost': np.zeros((X.shape[0], len(np.unique(y)))),
        'catboost': np.zeros((X.shape[0], len(np.unique(y))))
    }

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼
    test_preds = {
        'lightgbm': np.zeros((X_test.shape[0], len(np.unique(y)))),
        'xgboost': np.zeros((X_test.shape[0], len(np.unique(y)))),
        'catboost': np.zeros((X_test.shape[0], len(np.unique(y))))
    }

    # ê° ëª¨ë¸ë³„ ì´ í•™ìŠµ ì‹œê°„
    model_times = {
        'lightgbm': 0,
        'xgboost': 0,
        'catboost': 0
    }

    # Label Encoding
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    print(f"í•™ìŠµ ì‹œì‘: {n_folds}ê²¹ êµì°¨ ê²€ì¦")

    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y_encoded)):
        fold_start_time = time.time()
        print(f"\nFold {fold + 1}/{n_folds}")
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]

        # ìŠ¤ì¼€ì¼ë§ ì ìš©
        X_train_scaled, X_val_scaled, scaler = scale_features(X_train, X_val)
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ê°™ì€ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ë³€í™˜
        _, X_test_scaled, _ = scale_features(X_test, X_test, scaler=scaler)

        # LightGBM
        print("LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
        lgb_start = time.time()

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •
        lgb_params = {
            'objective': 'multiclass',
            'num_class': len(np.unique(y)),
            'boosting_type': 'gbdt',
            'n_estimators': 2000,  # ì¦ê°€
            'learning_rate': 0.005,  # ê°ì†Œ
            'max_depth': 6,  # ì¡°ì •
            'num_leaves': 31,  # ì¡°ì •
            'min_child_samples': 20,
            'min_child_weight': 0.001,
            'subsample': 0.85,  # ì¡°ì •
            'colsample_bytree': 0.85,  # ì¡°ì •
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'random_state': random_state
        }

        lgb_model = lgb.LGBMClassifier(**lgb_params)

        lgb_model.fit(X_train_scaled, y_train)

        lgb_end = time.time()
        lgb_time = lgb_end - lgb_start
        model_times['lightgbm'] += lgb_time
        print(f"LightGBM ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {lgb_time:.2f}ì´ˆ")

        oof_preds['lightgbm'][val_idx] = lgb_model.predict_proba(X_val_scaled)
        test_preds['lightgbm'] += lgb_model.predict_proba(X_test_scaled) / n_folds

        # XGBoost
        print("XGBoost ëª¨ë¸ í•™ìŠµ ì¤‘...")
        xgb_start = time.time()

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •
        xgb_params = {
            'objective': 'multi:softprob',
            'num_class': len(np.unique(y)),
            'n_estimators': 2000,  # ì¦ê°€
            'learning_rate': 0.005,  # ê°ì†Œ
            'max_depth': 7,  # ì¦ê°€
            'min_child_weight': 3,  # ì¶”ê°€
            'subsample': 0.85,  # ì¡°ì •
            'colsample_bytree': 0.85,  # ì¡°ì •
            'gamma': 0.1,  # ì¶”ê°€
            'reg_alpha': 0.1,  # ì¶”ê°€
            'reg_lambda': 1.0,  # ì¶”ê°€
            'random_state': random_state
        }
        xgb_model = xgb.XGBClassifier(**xgb_params)

        xgb_model.fit(X_train_scaled, y_train)

        xgb_end = time.time()
        xgb_time = xgb_end - xgb_start
        model_times['xgboost'] += xgb_time
        print(f"XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {xgb_time:.2f}ì´ˆ")

        oof_preds['xgboost'][val_idx] = xgb_model.predict_proba(X_val_scaled)
        test_preds['xgboost'] += xgb_model.predict_proba(X_test_scaled) / n_folds

        # CatBoost
        print("CatBoost ëª¨ë¸ í•™ìŠµ ì¤‘...")
        cat_start = time.time()

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •
        cat_params = {
            'iterations': 2000,  # ì¦ê°€
            'learning_rate': 0.005,  # ê°ì†Œ
            'depth': 8,  # ì¦ê°€
            'l2_leaf_reg': 5,  # ì¡°ì •
            'random_seed': random_state,
            'verbose': False,
            'bootstrap_type': 'Bernoulli',  # ì¶”ê°€
            'subsample': 0.85,  # ì¶”ê°€
        }
        cat_model = CatBoostClassifier(**cat_params)

        cat_model.fit(X_train_scaled, y_train)

        cat_end = time.time()
        cat_time = cat_end - cat_start
        model_times['catboost'] += cat_time
        print(f"CatBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {cat_time:.2f}ì´ˆ")

        oof_preds['catboost'][val_idx] = cat_model.predict_proba(X_val_scaled)
        test_preds['catboost'] += cat_model.predict_proba(X_test_scaled) / n_folds

        fold_end_time = time.time()
        print(f"Fold {fold + 1} ì´ ì†Œìš” ì‹œê°„: {fold_end_time - fold_start_time:.2f}ì´ˆ")

    # ê° ëª¨ë¸ë³„ OOF ì„±ëŠ¥ í‰ê°€
    model_accuracies = {}
    for model_name in oof_preds:
        oof_pred_labels = np.argmax(oof_preds[model_name], axis=1)
        oof_accuracy = accuracy_score(y_encoded, oof_pred_labels)
        model_accuracies[model_name] = oof_accuracy
        print(f"{model_name} OOF ì •í™•ë„: {oof_accuracy:.4f}, ì´ í•™ìŠµ ì‹œê°„: {model_times[model_name]:.2f}ì´ˆ")

    # ì•™ìƒë¸” - í‰ê· 
    ensemble_oof_preds = (oof_preds['lightgbm'] + oof_preds['xgboost'] + oof_preds['catboost']) / 3
    ensemble_oof_pred_labels = np.argmax(ensemble_oof_preds, axis=1)
    ensemble_accuracy = accuracy_score(y_encoded, ensemble_oof_pred_labels)
    print(f"ì•™ìƒë¸” OOF ì •í™•ë„: {ensemble_accuracy:.4f}")

    # ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì— ê¸°ë°˜í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°
    total_accuracy = sum(model_accuracies.values())
    model_weights = {model: acc / total_accuracy for model, acc in model_accuracies.items()}

    print("ëª¨ë¸ ê°€ì¤‘ì¹˜:")
    for model_name, weight in model_weights.items():
        print(f"  - {model_name}: {weight:.4f}")

    # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸”
    weighted_ensemble_test_preds = (
            model_weights['lightgbm'] * test_preds['lightgbm'] +
            model_weights['xgboost'] * test_preds['xgboost'] +
            model_weights['catboost'] * test_preds['catboost']
    )

    # ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ
    print("ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì¤‘...")
    stacking_start = time.time()

    # ë©”íƒ€ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ OOF ì˜ˆì¸¡ê°’ ì¤€ë¹„
    meta_train = np.column_stack([
        oof_preds['lightgbm'].reshape(X.shape[0], -1),
        oof_preds['xgboost'].reshape(X.shape[0], -1),
        oof_preds['catboost'].reshape(X.shape[0], -1)
    ])

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ ì¤€ë¹„
    meta_test = np.column_stack([
        test_preds['lightgbm'].reshape(X_test.shape[0], -1),
        test_preds['xgboost'].reshape(X_test.shape[0], -1),
        test_preds['catboost'].reshape(X_test.shape[0], -1)
    ])

    # ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
    from sklearn.linear_model import LogisticRegression
    meta_model = LogisticRegression(max_iter=1000, multi_class='multinomial', random_state=random_state)
    meta_model.fit(meta_train, y_encoded)

    # ìŠ¤íƒœí‚¹ ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    stacking_test_preds = meta_model.predict_proba(meta_test)

    stacking_end = time.time()
    stacking_time = stacking_end - stacking_start
    print(f"ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {stacking_time:.2f}ì´ˆ")

    # ìµœì¢… ì•™ìƒë¸”: ê°€ì¤‘ì¹˜ ì•™ìƒë¸”ê³¼ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì¡°í•© (0.6:0.4 ë¹„ìœ¨)
    final_ensemble_preds = 0.6 * weighted_ensemble_test_preds + 0.4 * stacking_test_preds
    final_ensemble_pred_labels = np.argmax(final_ensemble_preds, axis=1)

    # ì—­ë³€í™˜í•˜ì—¬ ì›ë˜ ë ˆì´ë¸”ë¡œ ë³€í™˜
    final_ensemble_pred_classes = le.inverse_transform(final_ensemble_pred_labels)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
    test_data = test_df.copy()
    test_data['pred_label'] = final_ensemble_pred_classes

    # ID ë³„ë¡œ ê°€ì¥ ë§ì´ ì˜ˆì¸¡ëœ Segmentë¥¼ ì„ íƒí•˜ì—¬ ì œì¶œ íŒŒì¼ ìƒì„±
    submission = test_data.groupby("ID")["pred_label"] \
        .agg(lambda x: x.value_counts().idxmax()) \
        .reset_index()
    submission.columns = ["ID", "Segment"]

    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission.to_csv('base_submit.csv', index=False)
    print("ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: base_submit.csv")

    # ëª¨ë¸ ì €ì¥
    print("ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
    with open('lgb_model.pkl', 'wb') as f:
        pickle.dump(lgb_model, f)
    with open('xgb_model.pkl', 'wb') as f:
        pickle.dump(xgb_model, f)
    with open('cat_model.pkl', 'wb') as f:
        pickle.dump(cat_model, f)
    with open('meta_model.pkl', 'wb') as f:
        pickle.dump(meta_model, f)
    print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ: lgb_model.pkl, xgb_model.pkl, cat_model.pkl, meta_model.pkl")

    # ì „ì²´ í•™ìŠµ ì¢…ë£Œ ì‹œê°„ ë° ì´ ì†Œìš” ì‹œê°„
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    print(f"\nì „ì²´ ëª¨ë¸ë§ ê³¼ì • ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time / 60:.2f}ë¶„)")

    return submission, ensemble_accuracy

# ì‹¤í–‰ í™˜ê²½ ì„¤ì •
RELOAD_FROM_PARQUET = True  # ì „ì²˜ë¦¬ëœ íŒŒì¼ ì‚¬ìš© ì—¬ë¶€
SKIP_PREPROCESSING = False  # ëª¨ë¸ë§ ë‹¨ê³„ë§Œ ì‹¤í–‰ ì—¬ë¶€

# íŒŒì¼ ê²½ë¡œ ì •ì˜
FINAL_TRAIN_PATH = "final_train_data.parquet"
FINAL_TEST_PATH = "final_test_data.parquet"
PREPROCESSED_TRAIN_PATH = "preprocessed_train_data.parquet"
PREPROCESSED_TEST_PATH = "preprocessed_test_data.parquet"

# ì´ë¯¸ ëª¨ë¸ë§ ì¤€ë¹„ê°€ ì™„ë£Œëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
if os.path.exists(FINAL_TRAIN_PATH) and os.path.exists(FINAL_TEST_PATH) and SKIP_PREPROCESSING:
    print(f"ëª¨ë¸ë§ ì¤€ë¹„ê°€ ì™„ë£Œëœ íŒŒì¼ ë°œê²¬: {FINAL_TRAIN_PATH}, {FINAL_TEST_PATH}")
    print("ëª¨ë¸ë§ìš© íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

    train_df = pd.read_parquet(FINAL_TRAIN_PATH)
    X = train_df.drop(columns=["ID", "Segment"])
    y = train_df["Segment"]

    test_df = pd.read_parquet(FINAL_TEST_PATH)
    test_ids = test_df["ID"].copy()
    X_test = test_df.drop(columns=["ID"])

    print(f"ëª¨ë¸ë§ìš© ë°ì´í„° ë¡œë“œ ì™„ë£Œ! í›ˆë ¨ í”¼ì²˜: {X.shape}, í…ŒìŠ¤íŠ¸ í”¼ì²˜: {X_test.shape}")
else:
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    train_df, test_df = preprocess_data(RELOAD_FROM_PARQUET)

    # íŠ¹ì„±ê³¼ íƒ€ê¹ƒ ë¶„ë¦¬
    feature_cols = [col for col in train_df.columns if col not in ["ID", "Segment"]]
    X = train_df[feature_cols].copy()
    y = train_df["Segment"].copy()

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ID ì €ì¥
    test_ids = test_df["ID"].copy()
    X_test = test_df[feature_cols].copy()

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    X, X_test = handle_missing_values(X, X_test)

    # ë²”ì£¼í˜• ë³€ìˆ˜ ì‹ë³„
    categorical_features = X.select_dtypes(include=['object']).columns.tolist()

    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    if categorical_features:
        print(f"ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ì¤‘... {categorical_features}")
        X, X_test, _ = encode_categorical_features(X, X_test, categorical_features)

    # ëª¨ë¸ë§ìš© ë°ì´í„° ì €ì¥ (íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œëœ ë°ì´í„°)
    print("ëª¨ë¸ë§ìš© ë°ì´í„° ì €ì¥ ì¤‘...")
    final_train = pd.concat([pd.DataFrame({'ID': train_df['ID']}), X, pd.DataFrame({'Segment': y})], axis=1)
    final_test = pd.concat([pd.DataFrame({'ID': test_df['ID']}), X_test], axis=1)

    final_train.to_parquet(FINAL_TRAIN_PATH, index=False)
    final_test.to_parquet(FINAL_TEST_PATH, index=False)
    print(f"ëª¨ë¸ë§ìš© ë°ì´í„° ì €ì¥ ì™„ë£Œ! íŒŒì¼ëª…: {FINAL_TRAIN_PATH}, {FINAL_TEST_PATH}")

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
submission, accuracy = train_and_evaluate_models(X, y, X_test, test_ids, test_df)
print(f"ìµœì¢… ëª¨ë¸ ì •í™•ë„: {accuracy:.4f}")

# ì œì¶œ ì–‘ì‹ í™•ì¸
print(f"ì œì¶œ íŒŒì¼ í¬ê¸°: {submission.shape}")