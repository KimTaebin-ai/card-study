{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ],
   "id": "c081601b291d2150"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import xgboost as xgb",
   "id": "1cee4f8bd2249735"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 메모리 사용량 확인 및 정리",
   "id": "111f0de741b54e73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "def check_memory_usage():\n",
    "    memory_usage = pd.DataFrame(columns=['Name', 'Memory Usage (MB)'])\n",
    "    for var_name in dir():\n",
    "        if var_name.startswith('_') or var_name in ['memory_usage', 'check_memory_usage', 'gc', 'pd', 'np']:\n",
    "            continue\n",
    "        var = globals()[var_name]\n",
    "        if isinstance(var, pd.DataFrame):\n",
    "            memory_mb = var.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "            temp_df = pd.DataFrame([[var_name, f\"{memory_mb:.2f}\"]], columns=['Name', 'Memory Usage (MB)'])\n",
    "            memory_usage = pd.concat([memory_usage, temp_df], ignore_index=True)\n",
    "\n",
    "    memory_usage = memory_usage.sort_values(by='Memory Usage (MB)', ascending=False).reset_index(drop=True)\n",
    "    print(memory_usage.head(10))\n",
    "    total_memory = float(memory_usage['Memory Usage (MB)'].str.replace(',', '').astype(float).sum())\n",
    "    print(f\"Total Memory Usage: {total_memory:.2f} MB\")"
   ],
   "id": "7c7e76d606dfddf3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 파일 불러오기",
   "id": "8a080f97ef8a86c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_data(data_type):\n",
    "    dfs = {}\n",
    "    base_path = f\"/kaggle/input/credit-card-segment/open/{data_type}\"\n",
    "    categories = {\n",
    "        \"customer\": \"1\",\n",
    "        \"credit\": \"2\",\n",
    "        \"sales\": \"3\",\n",
    "        \"billing\": \"4\",\n",
    "        \"balance\": \"5\",\n",
    "        \"channel\": \"6\",\n",
    "        \"marketing\": \"7\",\n",
    "        \"performance\": \"8\"\n",
    "    }\n",
    "    for name, prefix in categories.items():\n",
    "        path =  f\"{base_path}/{prefix}.*/*.parquet\"\n",
    "        files = sorted(glob.glob(path))\n",
    "        if not files:\n",
    "            print(f\"No parquet files found in {path}\")\n",
    "            continue\n",
    "        df_list = []\n",
    "        for f in files:\n",
    "            temp_df = pd.read_parquet(f)\n",
    "\n",
    "            # 메모리 최적화: 데이터 타입 변환\n",
    "            for col in temp_df.columns:\n",
    "                if temp_df[col].dtype == 'float64' and temp_df[col].fillna(0).apply(lambda x: x.is_integer() if isinstance(x, float) else True).all():\n",
    "                    temp_df[col] = temp_df[col].astype('float32')\n",
    "                elif temp_df[col].dtype == 'int64':\n",
    "                    temp_df[col] = temp_df[col].astype('int32')\n",
    "\n",
    "            df_list.append(temp_df)\n",
    "            del temp_df\n",
    "            gc.collect()\n",
    "\n",
    "        dfs[name] = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"{name} Loaded {dfs[name].shape} parquet files\")\n",
    "\n",
    "        # 각 카테고리 로드 후 메모리 정리\n",
    "        del df_list\n",
    "        gc.collect()\n",
    "    return dfs"
   ],
   "id": "cfcc38269c6570cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import gc",
   "id": "ab3202dc107b16c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import re",
   "id": "5f870417ab4c5e00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 전처리 함수들\n",
    "1. '개, 대, 회, 이상, 회이상, 개이상, 대이상, 대 이상, 회 이상'로 끝나는 문자열에서 숫자만 추출\n",
    "2. '', 'none', 'null', 'nan', 'NaN' 이 문자열로 들어왔을 때 처리, 10101, -999999, -99, 999, 99999999 값 0 처리\n",
    "3. unique 값이 1개인 컬럼 drop\n",
    "4. IQR 방식으로 이상치 탐색 후 NaN 으로 대치 (날짜 형태의 컬럼은 0으로 처리)\n",
    "5. 날짜형 데이터의 추가적인 결측치가 있을 경우 0처리\n",
    "6. 각 컬럼별 결측치의 비율에 따라 평균값, 중앙값, 최빈값, drop등으로 처리"
   ],
   "id": "6a9f0c34dc9f3b96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 문자열 -> 수치형으로 파싱\n",
    "\n",
    "'개, 대, 회, 이상, 회이상, 개이상, 대이상'로 끝나는 문자열에서 숫자만 추출\n"
   ],
   "id": "b1dfecf8ef1f556f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_units_from_dataframe(df):\n",
    "    # 데이터프레임 복사본 생성\n",
    "    df_cleaned = df.copy()\n",
    "    object_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
    "    # 각 object 컬럼 처리\n",
    "    for col in object_columns:\n",
    "        if df_cleaned[col].isnull().all():  # 모든 값이 null이면 처리 안함\n",
    "            continue\n",
    "\n",
    "        # 문자열인 경우에만 처리\n",
    "        mask = df_cleaned[col].apply(lambda x: isinstance(x, str))\n",
    "\n",
    "        if mask.any():  # 문자열이 하나 이상 있는 경우만 처리\n",
    "            unit_pattern = r'\\d+\\s*(개|대|회|이상|회이상|개이상|대이상|회 이상|일 이상)$'\n",
    "            # re.search를 직접 적용하여 경고 피하기\n",
    "            unit_mask = df_cleaned.loc[mask, col].apply(lambda x: bool(re.search(unit_pattern, x)))\n",
    "\n",
    "            # 단위가 있는 값만 처리\n",
    "            if unit_mask.any():\n",
    "                # 숫자 추출하기 위한 패턴\n",
    "                number_pattern = r'(\\d+)\\s*(개|대|회|이상|회이상|개이상|대이상|회 이상|일 이상)$'\n",
    "                # 숫자 부분만 추출\n",
    "                extracted_numbers = df_cleaned.loc[mask, col].str.extract(number_pattern)[0]\n",
    "                # 추출된 숫자를 새로운 값으로 사용 (숫자로 변환)\n",
    "                df_cleaned.loc[mask & unit_mask, col] = pd.to_numeric(\n",
    "                    extracted_numbers.loc[unit_mask], errors='coerce')\n",
    "\n",
    "    # 메모리 최적화\n",
    "    gc.collect()\n",
    "    return df_cleaned"
   ],
   "id": "f68e83ebe4da9969"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 결측치 수기 처리\n",
    "\n",
    "'', 'none', 'null', 'nan', 'NaN' 이 문자열로 들어왔을 때 처리, 10101, -999999, -99, 999, 99999999 값 0 처리"
   ],
   "id": "93667b12c8025790"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def is_special_null(df):\n",
    "    # 원본 데이터프레임 수정하기\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        # 문자열 컬럼에서 빈 문자열, 'none', 'null', 'nan' 찾기\n",
    "        mask = df[col].astype(str).str.lower().isin(['', 'none', 'null', 'nan', 'NaN'])\n",
    "        df.loc[mask, col] = np.nan  # NaN으로 변경\n",
    "\n",
    "    # 숫자 컬럼 처리\n",
    "    for col in df.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns:\n",
    "        # 특정 숫자값 찾기\n",
    "        mask = df[col].isin([10101, -999999, -99, 999, 99999999])\n",
    "        df.loc[mask, col] = np.nan  # NaN으로 변경\n",
    "\n",
    "    gc.collect()\n",
    "    return df"
   ],
   "id": "697777ef6a47705f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 컬럼별 데이터 형태가 1개밖에 없는 경우 drop\n",
    "\n",
    "unique 값이 1개인 컬럼 drop\n"
   ],
   "id": "760c21b4efb1625f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "def remove_single_value_columns(df):\n",
    "    single_value_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == 1:\n",
    "            single_value_cols.append(col)\n",
    "\n",
    "    if single_value_cols:\n",
    "        print(f\"제거할 컬럼 (유니크 값 1개): {single_value_cols}\")\n",
    "        df = df.drop(columns=single_value_cols)\n",
    "    else:\n",
    "        print(\"유니크 값이 1개인 컬럼이 없습니다.\")\n",
    "\n",
    "    gc.collect()\n",
    "    return df"
   ],
   "id": "e22c8245fa830da8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 이상치 처리\n",
    "\n",
    "IQR 방식으로 이상치 탐색 후 NaN 으로 대치 (날짜 형태의 컬럼은 0으로 처리)\n",
    "\n",
    "날짜형 데이터의 추가적인 결측치가 있을 경우 0처리"
   ],
   "id": "5596ddf843b56abb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 이상치 처리 함수 최적화\n",
    "def handle_outliers_with_iqr(df, date_columns=None):\n",
    "    if date_columns is None:\n",
    "        date_columns = []\n",
    "\n",
    "    # 모든 수치형 컬럼 선택\n",
    "    numeric_cols = df.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns\n",
    "\n",
    "    # 각 수치형 컬럼 처리\n",
    "    for col in numeric_cols:\n",
    "        # 결측치가 아닌 값으로만 IQR 계산\n",
    "        valid_data = df[col].dropna()\n",
    "\n",
    "        if len(valid_data) > 0:  # 유효한 데이터가 있는 경우만 처리\n",
    "            Q1 = valid_data.quantile(0.25)\n",
    "            Q3 = valid_data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # 이상치 경계 계산\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # 이상치 찾기\n",
    "            mask_lower = df[col] < lower_bound\n",
    "            mask_upper = df[col] > upper_bound\n",
    "\n",
    "            # 이상치 개수 출력\n",
    "            outliers_count = mask_lower.sum() + mask_upper.sum()\n",
    "            if outliers_count > 0:\n",
    "                print(f\"컬럼 '{col}': {outliers_count}개의 이상치 발견\")\n",
    "\n",
    "            # 이상치 처리 - date_columns에 포함된 컬럼이면 0으로, 아니면 NaN으로\n",
    "            if col in date_columns:\n",
    "                df.loc[mask_lower | mask_upper, col] = 0\n",
    "                if outliers_count > 0:\n",
    "                    print(f\"  -> 날짜 컬럼으로 간주하여 이상치를 0으로 대체\")\n",
    "            else:\n",
    "                df.loc[mask_lower | mask_upper, col] = np.nan\n",
    "                if outliers_count > 0:\n",
    "                    print(f\"  -> 일반 컬럼으로 간주하여 이상치를 NaN으로 대체\")\n",
    "\n",
    "    gc.collect()\n",
    "    return df"
   ],
   "id": "df70001ce7779e75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def handle_date_columns(df, date_cols):\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            missing_before = df[col].isnull().sum()\n",
    "            df[col] = df[col].fillna(0)\n",
    "            missing_after = df[col].isnull().sum()\n",
    "            print(f\"컬럼 '{col}'의 결측치: {missing_before} -> {missing_after}\")\n",
    "\n",
    "    gc.collect()\n",
    "    return df"
   ],
   "id": "e4e8aaa00c019c14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 결측치 처리\n",
    "\n",
    "각 컬럼별 결측치의 비율에 따라 평균값, 중앙값, 최빈값, drop등으로 처리"
   ],
   "id": "3d861c6b477282e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def handle_missing_values(df):\n",
    "    # 결측치가 있는 컬럼 찾기\n",
    "    columns_with_missing = [col for col in df.columns if df[col].isnull().sum() > 0]\n",
    "\n",
    "    for col in columns_with_missing:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_ratio = missing_count / len(df)\n",
    "        print(f\"컬럼 '{col}'의 결측치: {missing_count}개 ({missing_ratio:.4f})\")\n",
    "\n",
    "        # 결측 비율에 따른 처리\n",
    "        if missing_ratio > 0.75:\n",
    "            # 75% 초과 결측: 컬럼 제거\n",
    "            print(f\"  -> 결측치 비율이 75%를 초과합니다. 해당 컬럼 제거\")\n",
    "            df = df.drop(columns=[col])\n",
    "\n",
    "        elif missing_ratio > 0.5:\n",
    "            # 50~75% 결측\n",
    "            if df[col].dtype in ['int32', 'int64', 'float32', 'float64']:\n",
    "                median_val = df[col].median()\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "                print(f\"  -> 결측 비율이 높은 수치형 컬럼이므로 중앙값으로 대체\")\n",
    "            else:\n",
    "                mode_val = df[col].mode()[0] if not df[col].mode().empty else \"unknown\"\n",
    "                df[col] = df[col].fillna(mode_val)\n",
    "                print(f\"  -> 결측 비율이 높은 범주형 컬럼이므로 최빈값으로 대체\")\n",
    "\n",
    "        elif missing_ratio > 0.3:\n",
    "            # 30~50% 결측\n",
    "            if df[col].dtype in ['int32', 'int64', 'float32', 'float64']:\n",
    "                try:\n",
    "                    skew = df[col].skew()\n",
    "                    if abs(skew) > 1:  # 비대칭 분포인 경우\n",
    "                        median_val = df[col].median()\n",
    "                        df[col] = df[col].fillna(median_val)\n",
    "                        print(f\"  -> 분포가 비대칭이므로 중앙값으로 대체\")\n",
    "                    else:  # 대칭 분포인 경우\n",
    "                        mean_val = df[col].mean()\n",
    "                        df[col] = df[col].fillna(mean_val)\n",
    "                        print(f\"  -> 분포가 대칭이므로 평균값으로 대체\")\n",
    "                except:\n",
    "                    # 왜도 계산 오류 시 중앙값 사용\n",
    "                    median_val = df[col].median()\n",
    "                    df[col] = df[col].fillna(median_val)\n",
    "                    print(f\"  -> 중앙값으로 대체\")\n",
    "            else:\n",
    "                mode_val = df[col].mode()[0] if not df[col].mode().empty else \"unknown\"\n",
    "                df[col] = df[col].fillna(mode_val)\n",
    "                print(f\"  -> 범주형 컬럼이므로 최빈값으로 대체\")\n",
    "\n",
    "        else:\n",
    "            # 30% 이하 결측\n",
    "            if df[col].dtype in ['int32', 'int64', 'float32', 'float64']:\n",
    "                try:\n",
    "                    skew = df[col].skew()\n",
    "                    if abs(skew) > 1:\n",
    "                        median_val = df[col].median()\n",
    "                        df[col] = df[col].fillna(median_val)\n",
    "                        print(f\"  -> 분포가 비대칭이므로 중앙값으로 대체\")\n",
    "                    else:\n",
    "                        mean_val = df[col].mean()\n",
    "                        df[col] = df[col].fillna(mean_val)\n",
    "                        print(f\"  -> 분포가 대칭이므로 평균값으로 대체\")\n",
    "                except:\n",
    "                    median_val = df[col].median()\n",
    "                    df[col] = df[col].fillna(median_val)\n",
    "                    print(f\"  -> 중앙값으로 대체\")\n",
    "            else:\n",
    "                mode_val = df[col].mode()[0] if not df[col].mode().empty else \"unknown\"\n",
    "                df[col] = df[col].fillna(mode_val)\n",
    "                print(f\"  -> 범주형 컬럼이므로 최빈값으로 대체\")\n",
    "\n",
    "    gc.collect()\n",
    "    return df"
   ],
   "id": "a76462af12a7e42c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 라벨 인코딩 처리",
   "id": "1ea1cb9334efc073"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.preprocessing import LabelEncoder",
   "id": "baed85b5d52f6880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def label_encode_dataframe(df, exclude_cols=None):\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "\n",
    "    # object 타입 컬럼 찾기\n",
    "    object_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # 인코딩할 컬럼 (제외 컬럼 제외)\n",
    "    encode_columns = [col for col in object_columns if col not in exclude_cols]\n",
    "\n",
    "    if len(encode_columns) == 0:\n",
    "        print(\"인코딩할 object 타입 컬럼이 없습니다.\")\n",
    "        return df\n",
    "\n",
    "    print(f\"총 {len(encode_columns)}개의 object 타입 컬럼에 라벨 인코딩을 적용합니다.\")\n",
    "\n",
    "    # 각 컬럼에 라벨 인코딩 적용\n",
    "    for col in encode_columns:\n",
    "        print(f\"컬럼 '{col}' 인코딩 중...\")\n",
    "\n",
    "        # 해당 컬럼의 유니크 값 수 확인\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"  -> 유니크 값 수: {unique_count}\")\n",
    "\n",
    "        # 누락된 값이 있는지 확인\n",
    "        null_count = df[col].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            print(f\"  -> 주의: {null_count}개의 결측치가 있습니다.\")\n",
    "\n",
    "        # 라벨 인코더 생성 및 적용\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        # 결측치가 있는 경우 임시로 처리 (인코딩 후 다시 NaN으로 변경)\n",
    "        temp_col = df[col].copy()\n",
    "        mask_null = temp_col.isnull()\n",
    "\n",
    "        if mask_null.any():\n",
    "            # 결측치는 임시로 특수 문자열로 대체\n",
    "            temp_col = temp_col.fillna('NULL_VALUE_FOR_ENCODING')\n",
    "\n",
    "        # 인코딩 적용\n",
    "        temp_col = le.fit_transform(temp_col.astype(str))\n",
    "\n",
    "        # 인코딩된 값 데이터프레임에 대입\n",
    "        df[col] = temp_col\n",
    "\n",
    "        # 결측치가 있었다면 다시 NaN으로 변경\n",
    "        if mask_null.any():\n",
    "            df.loc[mask_null, col] = np.nan\n",
    "\n",
    "        print(f\"  -> 인코딩 완료: {len(le.classes_)}개의 클래스로 변환\")\n",
    "\n",
    "        # 메모리 정리\n",
    "        del temp_col, le, mask_null\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"모든 컬럼 인코딩 완료!\")\n",
    "    return df"
   ],
   "id": "cd82773680b2ea28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "def process_data():\n",
    "    print(\"데이터 로드 시작...\")\n",
    "    train_df = load_data(\"train\")\n",
    "    test_df = load_data(\"test\")\n",
    "    print(\"데이터 로드 완료!\")\n",
    "\n",
    "    # 메모리 정리\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 데이터 선언\n",
    "    customer_train_df = train_df[\"customer\"]\n",
    "    credit_train_df = train_df[\"credit\"]\n",
    "    sales_train_df = train_df[\"sales\"]\n",
    "    billing_train_df = train_df[\"billing\"]\n",
    "    balance_train_df = train_df[\"balance\"]\n",
    "    channel_train_df = train_df[\"channel\"]\n",
    "    marketing_train_df = train_df[\"marketing\"]\n",
    "    performance_train_df = train_df[\"performance\"]\n",
    "\n",
    "    # 훈련 데이터 사전 정리\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    # 테스트 데이터 설정\n",
    "    customer_test_df = test_df[\"customer\"]\n",
    "    credit_test_df = test_df[\"credit\"]\n",
    "    sales_test_df = test_df[\"sales\"]\n",
    "    billing_test_df = test_df[\"billing\"]\n",
    "    balance_test_df = test_df[\"balance\"]\n",
    "    channel_test_df = test_df[\"channel\"]\n",
    "    marketing_test_df = test_df[\"marketing\"]\n",
    "    performance_test_df = test_df[\"performance\"]\n",
    "\n",
    "    # 테스트 데이터 사전 정리\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 각 데이터프레임 처리 - 단위 제거\n",
    "    print(\"\\n단위 제거 처리 중...\")\n",
    "\n",
    "    # 훈련 데이터 처리\n",
    "    for df_name in [\"customer_train_df\", \"credit_train_df\", \"sales_train_df\", \"billing_train_df\",\n",
    "                    \"balance_train_df\", \"channel_train_df\", \"marketing_train_df\", \"performance_train_df\"]:\n",
    "        print(f\"{df_name} 단위 제거 중...\")\n",
    "        globals()[df_name] = clean_units_from_dataframe(globals()[df_name])\n",
    "\n",
    "    # 테스트 데이터 처리\n",
    "    for df_name in [\"customer_test_df\", \"credit_test_df\", \"sales_test_df\", \"billing_test_df\",\n",
    "                    \"balance_test_df\", \"channel_test_df\", \"marketing_test_df\", \"performance_test_df\"]:\n",
    "        print(f\"{df_name} 단위 제거 중...\")\n",
    "        globals()[df_name] = clean_units_from_dataframe(globals()[df_name])\n",
    "\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 특수 null 값 처리\n",
    "    print(\"\\n특수 null 값 처리 중...\")\n",
    "\n",
    "    # 훈련 데이터 처리\n",
    "    for df_name in [\"customer_train_df\", \"credit_train_df\", \"sales_train_df\", \"billing_train_df\",\n",
    "                    \"balance_train_df\", \"channel_train_df\", \"marketing_train_df\", \"performance_train_df\"]:\n",
    "        print(f\"{df_name} 특수 null 값 처리 중...\")\n",
    "        globals()[df_name] = is_special_null(globals()[df_name])\n",
    "\n",
    "    # 테스트 데이터 처리\n",
    "    for df_name in [\"customer_test_df\", \"credit_test_df\", \"sales_test_df\", \"billing_test_df\",\n",
    "                    \"balance_test_df\", \"channel_test_df\", \"marketing_test_df\", \"performance_test_df\"]:\n",
    "        print(f\"{df_name} 특수 null 값 처리 중...\")\n",
    "        globals()[df_name] = is_special_null(globals()[df_name])\n",
    "\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 단일 값 컬럼 제거\n",
    "    print(\"\\n단일 값 컬럼 제거 중...\")\n",
    "\n",
    "    # 훈련 데이터 처리\n",
    "    for df_name in [\"customer_train_df\", \"credit_train_df\", \"sales_train_df\", \"billing_train_df\",\n",
    "                    \"balance_train_df\", \"channel_train_df\", \"marketing_train_df\", \"performance_train_df\"]:\n",
    "        print(f\"{df_name} 단일 값 컬럼 제거 중...\")\n",
    "        globals()[df_name] = remove_single_value_columns(globals()[df_name])\n",
    "\n",
    "    # 테스트 데이터 처리\n",
    "    for df_name in [\"customer_test_df\", \"credit_test_df\", \"sales_test_df\", \"billing_test_df\",\n",
    "                    \"balance_test_df\", \"channel_test_df\", \"marketing_test_df\", \"performance_test_df\"]:\n",
    "        print(f\"{df_name} 단일 값 컬럼 제거 중...\")\n",
    "        globals()[df_name] = remove_single_value_columns(globals()[df_name])\n",
    "\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 날짜 컬럼 정의\n",
    "    customer_date_columns = [\"입회일자_신용\", \"최종유효년월_신용_이용가능\", \"최종유효년월_신용_이용\", \"최종카드발급일자\"]\n",
    "    sales_date_columns = [\"최종이용일자_기본\", \"최종이용일자_신판\", \"최종이용일자_CA\", \"최종이용일자_카드론\",\n",
    "                           \"최종이용일자_체크\", \"최종이용일자_일시불\", \"최종이용일자_할부\", \"최종카드론_대출일자\"]\n",
    "\n",
    "    # 이상치 처리\n",
    "    print(\"\\n이상치 처리 중...\")\n",
    "\n",
    "    # 훈련 데이터 이상치 처리\n",
    "    print(\"customer_train_df 이상치 처리 중...\")\n",
    "    customer_train_df = handle_outliers_with_iqr(customer_train_df, customer_date_columns)\n",
    "\n",
    "    for df_name, date_cols in [\n",
    "        (\"credit_train_df\", None),\n",
    "        (\"sales_train_df\", sales_date_columns),\n",
    "        (\"billing_train_df\", None),\n",
    "        (\"balance_train_df\", None),\n",
    "        (\"channel_train_df\", None),\n",
    "        (\"marketing_train_df\", None),\n",
    "        (\"performance_train_df\", None)\n",
    "    ]:\n",
    "        print(f\"{df_name} 이상치 처리 중...\")\n",
    "        globals()[df_name] = handle_outliers_with_iqr(globals()[df_name], date_cols)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # 테스트 데이터 이상치 처리\n",
    "    print(\"\\n테스트 데이터 이상치 처리 중...\")\n",
    "    print(\"customer_test_df 이상치 처리 중...\")\n",
    "    customer_test_df = handle_outliers_with_iqr(customer_test_df, customer_date_columns)\n",
    "\n",
    "    for df_name, date_cols in [\n",
    "        (\"credit_test_df\", None),\n",
    "        (\"sales_test_df\", sales_date_columns),\n",
    "        (\"billing_test_df\", None),\n",
    "        (\"balance_test_df\", None),\n",
    "        (\"channel_test_df\", None),\n",
    "        (\"marketing_test_df\", None),\n",
    "        (\"performance_test_df\", None)\n",
    "    ]:\n",
    "        print(f\"{df_name} 이상치 처리 중...\")\n",
    "        globals()[df_name] = handle_outliers_with_iqr(globals()[df_name], date_cols)\n",
    "\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 날짜 컬럼 처리\n",
    "    print(\"\\n날짜 컬럼 처리 중...\")\n",
    "    customer_train_df = handle_date_columns(customer_train_df, customer_date_columns)\n",
    "    sales_train_df = handle_date_columns(sales_train_df, sales_date_columns)\n",
    "    customer_test_df = handle_date_columns(customer_test_df, customer_date_columns)\n",
    "    sales_test_df = handle_date_columns(sales_test_df, sales_date_columns)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # 결측치 처리\n",
    "    print(\"\\n결측치 처리 중...\")\n",
    "\n",
    "    # 훈련 데이터 결측치 처리\n",
    "    for df_name in [\"customer_train_df\", \"credit_train_df\", \"sales_train_df\", \"billing_train_df\",\n",
    "                    \"balance_train_df\", \"channel_train_df\", \"marketing_train_df\", \"performance_train_df\"]:\n",
    "        print(f\"{df_name} 결측치 처리 중...\")\n",
    "        globals()[df_name] = handle_missing_values(globals()[df_name])\n",
    "\n",
    "    # 테스트 데이터 결측치 처리\n",
    "    for df_name in [\"customer_test_df\", \"credit_test_df\", \"sales_test_df\", \"billing_test_df\",\n",
    "                    \"balance_test_df\", \"channel_test_df\", \"marketing_test_df\", \"performance_test_df\"]:\n",
    "        print(f\"{df_name} 결측치 처리 중...\")\n",
    "        globals()[df_name] = handle_missing_values(globals()[df_name])\n",
    "\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 라벨 인코딩\n",
    "    print(\"\\n라벨 인코딩 중...\")\n",
    "\n",
    "    # 훈련 데이터 라벨 인코딩\n",
    "    exclude_columns = ['ID', 'Segment']\n",
    "    for df_name in [\"customer_train_df\", \"credit_train_df\", \"sales_train_df\", \"billing_train_df\",\n",
    "                    \"balance_train_df\", \"channel_train_df\", \"marketing_train_df\", \"performance_train_df\"]:\n",
    "        print(f\"{df_name} 라벨 인코딩 중...\")\n",
    "        globals()[df_name] = label_encode_dataframe(globals()[df_name], exclude_columns)\n",
    "\n",
    "    # 테스트 데이터 라벨 인코딩\n",
    "    exclude_columns = ['ID']\n",
    "    for df_name in [\"customer_test_df\", \"credit_test_df\", \"sales_test_df\", \"billing_test_df\",\n",
    "                    \"balance_test_df\", \"channel_test_df\", \"marketing_test_df\", \"performance_test_df\"]:\n",
    "        print(f\"{df_name} 라벨 인코딩 중...\")\n",
    "        globals()[df_name] = label_encode_dataframe(globals()[df_name], exclude_columns)\n",
    "\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 데이터 병합\n",
    "    print(\"\\n데이터 병합 중...\")\n",
    "\n",
    "    # 훈련 데이터 병합\n",
    "    print(\"훈련 데이터 병합 중...\")\n",
    "    merged_train_df = customer_train_df.copy()\n",
    "\n",
    "    for df_name in ['credit_train_df', 'sales_train_df', 'billing_train_df', 'balance_train_df',\n",
    "                    'channel_train_df', 'marketing_train_df', 'performance_train_df']:\n",
    "        print(f\"{df_name} 병합 중...\")\n",
    "        try:\n",
    "            # '기준년월'과 'ID' 모두를 기준으로 병합\n",
    "            if '기준년월' in globals()[df_name].columns and '기준년월' in merged_train_df.columns:\n",
    "                merged_train_df = merged_train_df.merge(globals()[df_name], on=['기준년월', 'ID'], how='left')\n",
    "            else:\n",
    "                merged_train_df = merged_train_df.merge(globals()[df_name], on='ID', how='left')\n",
    "        except Exception as e:\n",
    "            print(f\"병합 에러 발생: {e}\")\n",
    "            print(f\"병합 중단: {df_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"현재 병합된 데이터프레임 크기: {merged_train_df.shape}\")\n",
    "\n",
    "        # 병합 후 원본 데이터프레임 삭제\n",
    "        del globals()[df_name]\n",
    "        gc.collect()\n",
    "\n",
    "    # 테스트 데이터 병합\n",
    "    print(\"\\n테스트 데이터 병합 중...\")\n",
    "    merged_test_df = customer_test_df.copy()\n",
    "\n",
    "    for df_name in ['credit_test_df', 'sales_test_df', 'billing_test_df', 'balance_test_df',\n",
    "                    'channel_test_df', 'marketing_test_df', 'performance_test_df']:\n",
    "        print(f\"{df_name} 병합 중...\")\n",
    "        try:\n",
    "            # '기준년월'과 'ID' 모두를 기준으로 병합\n",
    "            if '기준년월' in globals()[df_name].columns and '기준년월' in merged_test_df.columns:\n",
    "                merged_test_df = merged_test_df.merge(globals()[df_name], on=['기준년월', 'ID'], how='left')\n",
    "            else:\n",
    "                merged_test_df = merged_test_df.merge(globals()[df_name], on='ID', how='left')\n",
    "        except Exception as e:\n",
    "            print(f\"병합 에러 발생: {e}\")\n",
    "            print(f\"병합 중단: {df_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"현재 병합된 데이터프레임 크기: {merged_test_df.shape}\")\n",
    "\n",
    "        # 병합 후 원본 데이터프레임 삭제\n",
    "        del globals()[df_name]\n",
    "        gc.collect()\n",
    "\n",
    "    # 고객 데이터프레임 제거\n",
    "    del customer_train_df, customer_test_df\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 모델 학습\n",
    "    print(\"\\nXGBoost 모델 학습 중...\")\n",
    "\n",
    "    # 특성 및 타겟 분리\n",
    "    print(\"특성 및 타겟 준비 중...\")\n",
    "    feature_cols = [col for col in merged_train_df.columns if col not in [\"ID\", \"Segment\"]]\n",
    "    X = merged_train_df[feature_cols].copy()\n",
    "    y = merged_train_df[\"Segment\"].copy()\n",
    "\n",
    "    # 타겟 인코딩\n",
    "    le_target = LabelEncoder()\n",
    "    y_encoded = le_target.fit_transform(y)\n",
    "\n",
    "    # 메모리에서 merged_train_df 제거\n",
    "    del merged_train_df\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 모델 생성 및 학습\n",
    "    print(\"XGBoost 모델 학습 시작...\")\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,  # 데이터의 80%만 사용하여 메모리 절약 및 과적합 방지\n",
    "        colsample_bytree=0.8,  # 특성의 80%만 사용하여 메모리 절약 및 과적합 방지\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 모델 학습 - 10% 데이터로 먼저 테스트 (메모리 부족 방지)\n",
    "    print(\"모델 학습 테스트 중 (10% 데이터)...\")\n",
    "    sample_indices = np.random.choice(len(X), size=int(len(X)*0.1), replace=False)\n",
    "    X_sample = X.iloc[sample_indices]\n",
    "    y_sample = y_encoded[sample_indices]\n",
    "\n",
    "    model.fit(X_sample, y_sample)\n",
    "    del X_sample, y_sample\n",
    "    gc.collect()\n",
    "\n",
    "    # 전체 데이터로 모델 학습 (메모리 문제가 없는 경우)\n",
    "    try:\n",
    "        print(\"전체 데이터로 모델 학습 중...\")\n",
    "        model.fit(X, y_encoded)\n",
    "    except MemoryError:\n",
    "        print(\"메모리 부족으로 샘플링된 데이터로 모델 학습을 진행합니다.\")\n",
    "        sample_size = 0.5  # 50% 데이터만 사용\n",
    "        sample_indices = np.random.choice(len(X), size=int(len(X)*sample_size), replace=False)\n",
    "        X_sample = X.iloc[sample_indices]\n",
    "        y_sample = y_encoded[sample_indices]\n",
    "        model.fit(X_sample, y_sample)\n",
    "        del X_sample, y_sample\n",
    "\n",
    "    # X 제거\n",
    "    del X, y, y_encoded\n",
    "    gc.collect()\n",
    "    check_memory_usage()\n",
    "\n",
    "    # 예측\n",
    "    print(\"\\n테스트 데이터 예측 중...\")\n",
    "    X_test = merged_test_df[feature_cols].copy()\n",
    "\n",
    "    # 결측치 처리 (예측 에러 방지)\n",
    "    for col in X_test.columns:\n",
    "        if X_test[col].dtype in ['int32', 'int64', 'float32', 'float64']:\n",
    "            X_test[col] = X_test[col].fillna(0)\n",
    "        else:\n",
    "            X_test[col] = X_test[col].fillna(\"unknown\")\n",
    "\n",
    "    # ID 컬럼 저장\n",
    "    test_ids = merged_test_df['ID'].copy()\n",
    "\n",
    "    # merged_test_df 제거\n",
    "    del merged_test_df\n",
    "    gc.collect()\n",
    "\n",
    "    # 배치 예측 (메모리 부족 방지)\n",
    "    batch_size = 10000\n",
    "    total_samples = len(X_test)\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    all_predictions = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, total_samples)\n",
    "\n",
    "        print(f\"배치 {i+1}/{num_batches} 예측 중... (샘플 {start_idx}-{end_idx})\")\n",
    "        batch_pred = model.predict(X_test.iloc[start_idx:end_idx])\n",
    "        all_predictions.extend(batch_pred)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # 예측 결과를 원래 라벨로 변환\n",
    "    y_test_pred_labels = le_target.inverse_transform(all_predictions)\n",
    "\n",
    "    # 결과 데이터프레임 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_ids,\n",
    "        'pred_label': y_test_pred_labels\n",
    "    })\n",
    "\n",
    "    # ID별로 가장 많이 예측된 세그먼트 선택\n",
    "    final_submission = submission.groupby(\"ID\")[\"pred_label\"] \\\n",
    "                      .agg(lambda x: x.value_counts().idxmax()) \\\n",
    "                      .reset_index()\n",
    "    final_submission.columns = [\"ID\", \"Segment\"]\n",
    "\n",
    "    # 결과 저장\n",
    "    final_submission.to_csv('./submission.csv', index=False)\n",
    "    print(\"예측 완료! 제출 파일이 생성되었습니다.\")\n",
    "\n",
    "    return final_submission"
   ],
   "id": "ebe00671ab4e662b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "process_data()",
   "id": "bd30613f44dabf24"
  }
 ]
}
